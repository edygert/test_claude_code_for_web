<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Streaming Chat</title>
    <script crossorigin src="https://unpkg.com/react@18/umd/react.production.min.js"></script>
    <script crossorigin src="https://unpkg.com/react-dom@18/umd/react-dom.production.min.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        .input-container {
            margin-bottom: 20px;
        }
        input {
            width: 70%;
            padding: 10px;
            font-size: 16px;
        }
        input.recording {
            background-color: #fff3cd;
            border-color: #ffc107;
        }
        button {
            padding: 10px 20px;
            font-size: 16px;
            margin-left: 10px;
            cursor: pointer;
        }
        button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .response-container {
            border: 1px solid #ccc;
            padding: 20px;
            min-height: 200px;
            white-space: pre-wrap;
            background-color: #f9f9f9;
        }
        .interim-preview {
            border: 1px solid #ffc107;
            padding: 10px;
            margin-bottom: 10px;
            background-color: #fff3cd;
            min-height: 30px;
            font-style: italic;
            color: #856404;
        }
        .error {
            color: red;
        }
    </style>
</head>
<body>
    <div id="root"></div>

    <script type="text/babel">
        const { useState, useEffect, useRef } = React;

        function App() {
            const [input, setInput] = useState('');
            const [response, setResponse] = useState('');
            const [responseVersion, setResponseVersion] = useState(0);
            const [isLoading, setIsLoading] = useState(false);
            const [error, setError] = useState('');
            const [isRecording, setIsRecording] = useState(false);
            const [recognition, setRecognition] = useState(null);
            const [interimTranscript, setInterimTranscript] = useState('');
            const [ttsEnabled, setTtsEnabled] = useState(false);
            const [isSpeaking, setIsSpeaking] = useState(false);
            const finalTranscriptRef = useRef('');
            const pauseTimeoutRef = useRef(null);

            useEffect(() => {
                // Initialize speech recognition
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                
                if (SpeechRecognition) {
                    const recognitionInstance = new SpeechRecognition();
                    recognitionInstance.continuous = true;
                    recognitionInstance.interimResults = true;
                    recognitionInstance.lang = 'zh-TW';
                    recognitionInstance.maxAlternatives = 3;

                    recognitionInstance.onresult = (event) => {
                        let interim = '';

                        // Clear existing pause timeout
                        if (pauseTimeoutRef.current) {
                            clearTimeout(pauseTimeoutRef.current);
                        }

                        // Only process NEW results starting from resultIndex to avoid repetition
                        for (let i = event.resultIndex; i < event.results.length; i++) {
                            const transcript = event.results[i][0].transcript;

                            if (event.results[i].isFinal) {
                                // Accumulate final results in ref
                                finalTranscriptRef.current += transcript;
                            } else {
                                // Collect interim results
                                interim += transcript;
                            }
                        }

                        // Display accumulated finals + current interim
                        const fullTranscript = finalTranscriptRef.current + interim;
                        setInterimTranscript(fullTranscript);

                        // Set new pause timeout - if user pauses for 1 second, auto-transfer text
                        // Capture fullTranscript in closure to ensure we transfer everything
                        if (fullTranscript) {
                            pauseTimeoutRef.current = setTimeout(() => {
                                setInput(prev => prev + fullTranscript);
                                setInterimTranscript('');
                                finalTranscriptRef.current = '';
                            }, 1000);
                        }
                    };

                    recognitionInstance.onerror = (event) => {
                        setError(`Speech recognition error: ${event.error}`);
                        setIsRecording(false);
                        setInterimTranscript('');
                        finalTranscriptRef.current = '';
                        if (pauseTimeoutRef.current) {
                            clearTimeout(pauseTimeoutRef.current);
                            pauseTimeoutRef.current = null;
                        }
                    };

                    recognitionInstance.onend = () => {
                        setIsRecording(false);
                    };

                    setRecognition(recognitionInstance);
                }
            }, []);

            const handleStartRecording = () => {
                if (!recognition) {
                    setError('Speech recognition not supported in this browser');
                    return;
                }

                setError('');
                setInterimTranscript('');
                finalTranscriptRef.current = '';
                if (pauseTimeoutRef.current) {
                    clearTimeout(pauseTimeoutRef.current);
                    pauseTimeoutRef.current = null;
                }
                setIsRecording(true);
                recognition.start();
            };

            const handleStopRecording = () => {
                if (recognition) {
                    // Clear pause timeout
                    if (pauseTimeoutRef.current) {
                        clearTimeout(pauseTimeoutRef.current);
                        pauseTimeoutRef.current = null;
                    }

                    recognition.stop();
                    // Transfer accumulated transcript to input field
                    if (interimTranscript) {
                        setInput(prev => prev + interimTranscript);
                        setInterimTranscript('');
                        finalTranscriptRef.current = '';
                    }
                }
            };

            const speakResponse = (text) => {
                if (!ttsEnabled || !text) return;

                // Stop any ongoing speech
                window.speechSynthesis.cancel();

                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'zh-TW';  // Traditional Chinese
                utterance.rate = 1.0;
                utterance.pitch = 1.0;
                utterance.volume = 1.0;

                utterance.onstart = () => setIsSpeaking(true);
                utterance.onend = () => setIsSpeaking(false);
                utterance.onerror = () => setIsSpeaking(false);

                window.speechSynthesis.speak(utterance);
            };

            const stopSpeaking = () => {
                window.speechSynthesis.cancel();
                setIsSpeaking(false);
            };

            const handleSubmit = async (e) => {
                e.preventDefault();

                if (!input.trim()) return;

                // Stop any ongoing TTS
                stopSpeaking();

                setIsLoading(true);
                setError('');
                setResponse('');
                setResponseVersion(0);

                // Track timing for first chunk
                const requestStartTime = performance.now();
                let firstChunkTime = null;
                let fullResponseText = '';

                try {
                    const res = await fetch('http://localhost:8000/v1/chat/completions', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            messages: [
                                { role: 'user', content: input }
                            ],
                            max_tokens: 1024,
                            temperature: 0.7
                        })
                    });

                    if (!res.ok) {
                        throw new Error(`HTTP error! status: ${res.status}`);
                    }

                    const reader = res.body.getReader();
                    const decoder = new TextDecoder();

                    while (true) {
                        const { value, done } = await reader.read();
                        if (done) break;

                        const chunk = decoder.decode(value);
                        const lines = chunk.split('\n');

                        for (const line of lines) {
                            if (line.startsWith('data: ')) {
                                const data = line.slice(6);

                                if (data === '[DONE]') {
                                    // Log timing before returning
                                    const totalTime = performance.now() - requestStartTime;
                                    if (firstChunkTime) {
                                        console.log('TTFC:', (firstChunkTime - requestStartTime).toFixed(2), 'ms | Total:', totalTime.toFixed(2), 'ms');
                                    }
                                    setIsLoading(false);

                                    // Speak the response if TTS is enabled
                                    if (fullResponseText) {
                                        speakResponse(fullResponseText);
                                    }
                                    return;
                                }

                                try {
                                    const parsed = JSON.parse(data);

                                    if (parsed.error) {
                                        setError(parsed.error);
                                        setIsLoading(false);
                                        return;
                                    }

                                    // Handle different streaming formats
                                    let content = null;

                                    // OpenAI-style format: choices[0].delta.content
                                    if (parsed.choices && parsed.choices[0] && parsed.choices[0].delta) {
                                        content = parsed.choices[0].delta.content;
                                    }
                                    // Simple format: content
                                    else if (parsed.content) {
                                        content = parsed.content;
                                    }

                                    if (content) {
                                        // Record time to first chunk
                                        if (firstChunkTime === null) {
                                            firstChunkTime = performance.now();
                                        }

                                        // Accumulate full response for TTS
                                        fullResponseText += content;

                                        // Update response state and increment version to force render
                                        setResponse(prev => prev + content);
                                        setResponseVersion(v => v + 1);

                                        // Yield control to browser to allow painting
                                        await new Promise(resolve => setTimeout(resolve, 10));
                                    }
                                } catch (e) {
                                    // Skip malformed JSON
                                }
                            }
                        }
                    }
                } catch (err) {
                    setError(`Error: ${err.message}`);
                } finally {
                    setIsLoading(false);
                }
            };

            return (
                <div>
                    <h1>AI Streaming Chat</h1>
                    
                    <form onSubmit={handleSubmit} className="input-container">
                        <input
                            type="text"
                            value={input}
                            onChange={(e) => setInput(e.target.value)}
                            placeholder="Type your message..."
                            disabled={isLoading}
                            className={isRecording ? 'recording' : ''}
                        />
                        <button
                            type="button"
                            onClick={isRecording ? handleStopRecording : handleStartRecording}
                            disabled={isLoading}
                            style={{ backgroundColor: isRecording ? '#ff4444' : '#4CAF50' }}
                        >
                            {isRecording ? '‚èπ Stop' : 'üé§ Record'}
                        </button>
                        <button type="submit" disabled={isLoading || !input.trim()}>
                            {isLoading ? 'Sending...' : 'Send'}
                        </button>
                        <button
                            type="button"
                            onClick={() => setTtsEnabled(!ttsEnabled)}
                            style={{ backgroundColor: ttsEnabled ? '#2196F3' : '#757575' }}
                        >
                            {ttsEnabled ? 'üîä TTS On' : 'üîá TTS Off'}
                        </button>
                        {isSpeaking && (
                            <button
                                type="button"
                                onClick={stopSpeaking}
                                style={{ backgroundColor: '#ff4444' }}
                            >
                                ‚èπ Stop Speaking
                            </button>
                        )}
                    </form>

                    {isRecording && interimTranscript && (
                        <div className="interim-preview">
                            Preview: {interimTranscript}
                        </div>
                    )}

                    {error && <div className="error">{error}</div>}

                    <div className="response-container" key={responseVersion}>
                        {response || 'Response will appear here...'}
                    </div>
                </div>
            );
        }

        ReactDOM.render(<App />, document.getElementById('root'));
    </script>
</body>
</html>
